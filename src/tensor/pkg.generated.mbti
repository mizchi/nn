// Generated using `moon info`, DON'T EDIT IT
package "mizchi/nn/tensor"

// Values
pub fn accumulate_inplace(FixedArray[Float], FixedArray[Float], Int) -> Unit

pub fn adamw_config_default(Float) -> AdamwConfig

pub fn adamw_step_inplace(FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], Int, Float, Float, Float, Float, Float, Float, Float) -> Unit

pub fn add_matrix_inplace(FixedArray[Float], FixedArray[Float], Int, Int) -> Unit

pub fn add_matrix_inplace_offset(FixedArray[Float], Int, FixedArray[Float], Int, Int) -> Unit

pub fn add_positional_embedding(Tensor, Tensor) -> Tensor

pub fn attention(Tensor, Tensor, Tensor, Tensor?) -> Result[Tensor, String]

pub fn attention_backward_batch(FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], Int, Int, Int, Float) -> Unit

pub fn attention_backward_batch_interleaved(FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], Int, Int, Int, Int, Float) -> Unit

pub fn attention_forward_batch_interleaved(FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], Int, Int, Int, Int, Float) -> Unit

pub fn attention_forward_batch_interleaved_masked(FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], Int, Int, Int, Int, Float) -> Unit

pub fn attention_head_backward(FixedArray[Float], Int, FixedArray[Float], Int, FixedArray[Float], Int, FixedArray[Float], Int, FixedArray[Float], FixedArray[Float], Int, FixedArray[Float], Int, FixedArray[Float], Int, Int, Int, Float) -> Unit

pub fn batched_linear_backward_raw(FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], Int, Int, Int) -> Unit

pub fn bias_add_backward(FixedArray[Float], FixedArray[Float], Int, Int) -> Unit

pub fn bias_add_backward_arena(FixedArray[Float], Int, FixedArray[Float], Int, Int) -> Unit

pub fn bias_add_bwd_relu_dx(FixedArray[Float], Int, Int) -> Unit

pub fn bias_add_forward(FixedArray[Float], FixedArray[Float], FixedArray[Float], Int, Int) -> Unit

pub fn bias_add_inplace(FixedArray[Float], FixedArray[Float], Int, Int) -> Unit

pub fn bias_add_inplace_arena(FixedArray[Float], Int, FixedArray[Float], Int, Int) -> Unit

pub fn bias_add_relu_pre(FixedArray[Float], Int, Int) -> Unit

pub fn causal_mask(Int) -> Result[Tensor, String]

pub fn char_tokenizer_from_text(String) -> CharTokenizer

pub fn clock_ns() -> UInt64

pub fn cross_entropy_backward(Tensor, Array[Int]) -> Tensor

pub fn cross_entropy_forward_backward(Tensor, Array[Int]) -> (Float, Tensor)

pub fn cross_entropy_fwd_bwd(FixedArray[Float], FixedArray[Int], FixedArray[Float], Int, Int) -> Float

pub fn embedding(Array[Array[Int]], Tensor) -> Tensor

pub fn extract_patches(Tensor, VitConfig) -> Tensor

pub fn feed_forward(Tensor, Tensor, Tensor, Tensor, Tensor) -> Result[Tensor, String]

pub fn feed_forward_backward_reference(Tensor, TransformerBlockCache, TransformerBlockParams, TransformerBlockGrads) -> Tensor

pub fn fused_ffn_backward(FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], Int, Int, Int) -> Unit

pub fn fused_linear_relu_backward(FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], Int, Int, Int, Int) -> Unit

pub fn fused_two_layer_last_profile(FixedArray[UInt64]) -> Unit

pub fn fused_two_layer_relu_backward(FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], Int, Int, Int, Int) -> Unit

pub fn gelu_backward(Tensor, Tensor) -> Tensor

pub fn generate_greedy(TransformerParams, TransformerConfig, Array[Int], Int) -> Result[Array[Int], String]

pub fn generate_with_temperature(TransformerParams, TransformerConfig, Array[Int], Int, Float, Int) -> Result[Array[Int], String]

pub fn grad_buf_to_fixed(FixedArray[Float], Int) -> Unit

pub fn layer_norm_backward(Tensor, Tensor, FixedArray[Float], FixedArray[Float], Tensor, Tensor, Tensor) -> Tensor

pub fn layer_norm_with_cache(Tensor, Tensor, Tensor, Float) -> (Tensor, FixedArray[Float], FixedArray[Float])

pub fn mha_forward_batch_interleaved(FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], Int, Int, Int, Int, Float) -> Unit

pub fn mha_forward_batch_interleaved_masked(FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], FixedArray[Float], Int, Int, Int, Int, Float) -> Unit

pub fn multi_head_attention(Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Int, Tensor?) -> Result[Tensor, String]

pub fn prepend_cls_token(Tensor, Tensor, Int) -> Tensor

pub fn relu_backward_arena(FixedArray[Float], Int, Int, Int, Int) -> Unit

pub fn relu_backward_from_pre(FixedArray[Float], FixedArray[Float], Int) -> Unit

pub fn relu_backward_fully_managed(Int) -> Unit

pub fn relu_backward_hybrid(FixedArray[Float], FixedArray[Float], Int, Int, Int) -> Unit

pub fn relu_backward_inplace(FixedArray[Float], FixedArray[Float], FixedArray[Float], Int) -> Unit

pub fn relu_backward_managed(FixedArray[Float], Int) -> Unit

pub fn relu_forward(FixedArray[Float], FixedArray[Float], Int) -> Unit

pub fn relu_forward_arena(FixedArray[Float], Int, Int, Int) -> Unit

pub fn relu_forward_hybrid(FixedArray[Float], Int, FixedArray[Float], Int) -> Unit

pub fn relu_from_pre(FixedArray[Float], Int) -> Unit

pub fn relu_from_pre_cached(FixedArray[Float], Int) -> Unit

pub fn saxpy(Int, Float, FixedArray[Float], FixedArray[Float]) -> Unit

pub fn scale_inplace(FixedArray[Float], Float, Int) -> Unit

pub fn sgemm(Int, Int, Int, Int, Int, Float, FixedArray[Float], Int, FixedArray[Float], Int, Float, FixedArray[Float], Int) -> Unit

pub fn sgemm_offset(Int, Int, Int, Int, Int, Float, FixedArray[Float], Int, Int, FixedArray[Float], Int, Int, Float, FixedArray[Float], Int, Int) -> Unit

pub fn sgemm_relu_dx_a(Int, Int, Int, Int, Int, Float, Int, FixedArray[Float], Int, Float, FixedArray[Float], Int) -> Unit

pub fn sgemm_to_grad_buf(Int, Int, Int, Int, Int, Float, FixedArray[Float], Int, FixedArray[Float], Int, Int) -> Unit

pub fn sgemm_to_relu_pre(Int, Int, Int, Int, Int, Float, FixedArray[Float], Int, FixedArray[Float], Int, Int) -> Unit

pub fn shape_broadcast(Shape, Shape) -> Result[Shape, String]

pub fn shape_from(FixedArray[Int]) -> Result[Shape, String]

pub fn shape_matmul(Shape, Shape) -> Result[Shape, String]

pub fn shape_new(Array[Int]) -> Result[Shape, String]

pub fn shape_new_unchecked(Array[Int]) -> Shape

pub fn shape_transpose(Shape, Int, Int) -> Result[Shape, String]

pub fn softmax_backward_rows(FixedArray[Float], FixedArray[Float], FixedArray[Float], Int, Int) -> Unit

pub fn softmax_inplace(FixedArray[Float], Int, Int) -> Unit

pub fn softmax_inplace_offset(FixedArray[Float], Int, Int, Int) -> Unit

pub fn tensor_add(Tensor, Tensor) -> Result[Tensor, String]

pub fn tensor_argmax(Tensor) -> Int

pub fn tensor_cross_entropy(Tensor, Array[Int]) -> Result[Float, String]

pub fn tensor_from_array(Array[Float]) -> Tensor

pub fn tensor_full(Shape, Float) -> Tensor

pub fn tensor_gelu(Tensor) -> Tensor

pub fn tensor_layer_norm(Tensor, Tensor, Tensor, Float) -> Result[Tensor, String]

pub fn tensor_log_softmax(Tensor) -> Result[Tensor, String]

pub fn tensor_matmul(Tensor, Tensor) -> Result[Tensor, String]

pub fn tensor_matmul_2d(Tensor, Tensor) -> Result[Tensor, String]

pub fn tensor_mean(Tensor, Int) -> Result[Tensor, String]

pub fn tensor_mul(Tensor, Tensor) -> Result[Tensor, String]

pub fn tensor_new(Shape, Array[Float]) -> Result[Tensor, String]

pub fn tensor_new_fixed(Shape, FixedArray[Float]) -> Tensor

pub fn tensor_ones(Shape) -> Tensor

pub fn tensor_relu(Tensor) -> Tensor

pub fn tensor_scale(Tensor, Float) -> Tensor

pub fn tensor_softmax(Tensor) -> Result[Tensor, String]

pub fn tensor_sub(Tensor, Tensor) -> Result[Tensor, String]

pub fn tensor_sum(Tensor, Int) -> Result[Tensor, String]

pub fn tensor_zeros(Shape) -> Tensor

pub fn transformer_backward(TransformerCache, TransformerParams, TransformerConfig, TransformerGrads, Array[Int]) -> Float

pub fn transformer_backward_lm(TransformerCache, TransformerParams, TransformerConfig, TransformerGrads, Array[Array[Int]]) -> Float

pub fn transformer_block(Tensor, TransformerBlockParams, TransformerConfig, Tensor?) -> Result[Tensor, String]

pub fn transformer_config(Int, Int, Int, Int, Int, Int) -> Result[TransformerConfig, String]

pub fn transformer_eval_lm_ids(Array[Int], TransformerParams, TransformerConfig, Int, Int, Int) -> (Float, Float)

pub fn transformer_forward(Array[Array[Int]], TransformerParams, TransformerConfig, Tensor?) -> Result[Tensor, String]

pub fn transformer_forward_with_cache(Array[Array[Int]], TransformerParams, TransformerConfig, Tensor?) -> (Tensor, TransformerCache)

pub fn transformer_init_params(TransformerConfig, Int) -> TransformerParams

pub fn transformer_lm_checkpoint_from_bytes(TransformerConfig, Bytes) -> Result[TransformerLmCheckpoint, String]

pub fn transformer_lm_checkpoint_to_bytes(TransformerConfig, TransformerParams, TransformerAdamwState?, Int) -> Bytes

pub fn transformer_train(String, TransformerConfig, Int, Float, Int) -> Array[Float]

pub fn transformer_train_lm_profile_steps(String, TransformerConfig, Int, Int, Int, Float, Int) -> Array[TransformerLmStepMetric]

pub fn transformer_train_lm_profile_steps_adamw(String, TransformerConfig, Int, Int, Int, Float, Int) -> Array[TransformerLmStepMetric]

pub fn transformer_train_lm_profile_steps_ids(Array[Int], TransformerConfig, Int, Int, Int, Float, Int) -> Array[TransformerLmStepMetric]

pub fn transformer_train_lm_profile_steps_ids_adamw(Array[Int], TransformerConfig, Int, Int, Int, Float, Int) -> Array[TransformerLmStepMetric]

pub fn transformer_train_lm_profile_steps_ids_inplace(Array[Int], TransformerParams, TransformerGrads, TransformerConfig, Int, Int, Int, Float, Int) -> Array[TransformerLmStepMetric]

pub fn transformer_train_lm_profile_steps_ids_inplace_adamw(Array[Int], TransformerParams, TransformerGrads, TransformerAdamwState, TransformerConfig, Int, Int, Int, Float, Int) -> Array[TransformerLmStepMetric]

pub fn transformer_train_lm_steps(String, TransformerConfig, Int, Int, Float, Int) -> Array[Float]

pub fn transformer_train_lm_steps_adamw(String, TransformerConfig, Int, Int, Float, Int) -> Array[Float]

pub fn transformer_train_lm_steps_ids(Array[Int], TransformerConfig, Int, Int, Float, Int) -> Array[Float]

pub fn transformer_train_lm_steps_ids_adamw(Array[Int], TransformerConfig, Int, Int, Float, Int) -> Array[Float]

pub fn transformer_zero_adamw_state(TransformerConfig) -> TransformerAdamwState

pub fn transformer_zero_grads(TransformerConfig) -> TransformerGrads

pub fn vit_backward(VitCache, VitParams, VitConfig, VitGrads, Array[Int]) -> Float

pub fn vit_config(Int, Int, Int, Int, Int, Int, Int) -> Result[VitConfig, String]

pub fn vit_eval(Tensor, VitParams, VitConfig, Array[Int], Int) -> Result[(Float, Float), String]

pub fn vit_forward(Tensor, VitParams, VitConfig) -> Result[Tensor, String]

pub fn vit_forward_with_cache(Tensor, VitParams, VitConfig) -> (Tensor, VitCache)

pub fn vit_init_params(VitConfig, Int) -> VitParams

pub fn vit_profile_aggregate(Array[VitProfileResult]) -> VitProfileResult

pub fn vit_profile_result_print(VitProfileResult) -> Unit

pub fn vit_profile_step(Tensor, Array[Int], VitParams, VitGrads, VitConfig, Float) -> VitProfileResult

pub fn vit_sgd_update(VitParams, VitGrads, Float) -> Unit

pub fn vit_train(Tensor, Array[Int], Int, VitConfig, Int, Int, Float, Int) -> Array[Float]

pub fn vit_train_with_eval(Tensor, Array[Int], Int, VitConfig, Int, Int, Float, Int) -> (VitParams, Array[(Float, Float)])

pub fn vit_zero_grads(VitConfig) -> VitGrads

pub fn vit_zero_grads_inplace(VitGrads) -> Unit

pub fn zero_inplace(FixedArray[Float], Int) -> Unit

// Errors

// Types and methods
pub struct AdamwConfig {
  lr : Float
  beta1 : Float
  beta2 : Float
  eps : Float
  weight_decay : Float
}
pub impl Eq for AdamwConfig
pub impl Show for AdamwConfig

pub struct CharTokenizer {
  chars : Array[Char]
}
pub fn CharTokenizer::decode(Self, Array[Int]) -> String
pub fn CharTokenizer::encode(Self, String) -> Array[Int]
pub fn CharTokenizer::vocab_size(Self) -> Int

pub struct Shape {
  dims : Array[Int]
}
pub fn Shape::bytes(Self) -> Int
pub fn Shape::dim(Self, Int) -> Int
pub fn Shape::eq(Self, Self) -> Bool
pub fn Shape::ndim(Self) -> Int
pub fn Shape::numel(Self) -> Int
pub fn Shape::strides(Self) -> Array[Int]
pub fn Shape::to_string(Self) -> String
pub impl Show for Shape

pub struct Tensor {
  data : FixedArray[Float]
  shape : Shape
  offset : Int
  strides : Array[Int]
}
pub fn Tensor::at1(Self, Int) -> Float
pub fn Tensor::at2(Self, Int, Int) -> Float
pub fn Tensor::at3(Self, Int, Int, Int) -> Float
pub fn Tensor::contiguous(Self) -> Self
pub fn Tensor::dim(Self, Int) -> Int
pub fn Tensor::get(Self, Array[Int]) -> Float
pub fn Tensor::is_contiguous(Self) -> Bool
pub fn Tensor::ndim(Self) -> Int
pub fn Tensor::numel(Self) -> Int
pub fn Tensor::reshape(Self, Shape) -> Result[Self, String]
pub fn Tensor::set(Self, Array[Int], Float) -> Unit
pub fn Tensor::set2(Self, Int, Int, Float) -> Unit
pub fn Tensor::slice(Self, Int, Int) -> Result[Self, String]
pub fn Tensor::to_array(Self) -> Array[Float]
pub fn Tensor::to_string(Self) -> String
pub fn Tensor::transpose(Self, Int, Int) -> Result[Self, String]
pub impl Show for Tensor

pub struct TransformerAdamwState {
  mut step : Int
  mut beta1_pow : Float
  mut beta2_pow : Float
  m : TransformerGrads
  v : TransformerGrads
}

pub struct TransformerBlockCache {
  x_input : Tensor
  ln1_out : Tensor
  ln1_mean : FixedArray[Float]
  ln1_rstd : FixedArray[Float]
  q_proj : Tensor
  k_proj : Tensor
  v_proj : Tensor
  attn_weights : FixedArray[Float]
  attn_out_pre_wo : Tensor
  x_after_res1 : Tensor
  ln2_out : Tensor
  ln2_mean : FixedArray[Float]
  ln2_rstd : FixedArray[Float]
  ff_pre_gelu : Tensor
  ff_post_gelu : Tensor
}

pub struct TransformerBlockGrads {
  d_ln1_gamma : Tensor
  d_ln1_beta : Tensor
  d_w_q : Tensor
  d_w_k : Tensor
  d_w_v : Tensor
  d_w_o : Tensor
  d_ln2_gamma : Tensor
  d_ln2_beta : Tensor
  d_ff_w1 : Tensor
  d_ff_b1 : Tensor
  d_ff_w2 : Tensor
  d_ff_b2 : Tensor
}

pub(all) struct TransformerBlockParams {
  ln1_gamma : Tensor
  ln1_beta : Tensor
  w_q : Tensor
  w_k : Tensor
  w_v : Tensor
  w_o : Tensor
  ln2_gamma : Tensor
  ln2_beta : Tensor
  ff_w1 : Tensor
  ff_b1 : Tensor
  ff_w2 : Tensor
  ff_b2 : Tensor
}

pub struct TransformerCache {
  tokens : Array[Array[Int]]
  x_with_pos : Tensor
  block_caches : Array[TransformerBlockCache]
  hidden_before_final_ln : Tensor
  ln_final_mean : FixedArray[Float]
  ln_final_rstd : FixedArray[Float]
  ln_final_out : Tensor
  logits : Tensor
}

pub struct TransformerConfig {
  vocab_size : Int
  d_model : Int
  num_heads : Int
  num_layers : Int
  d_ff : Int
  max_seq_len : Int
  eps : Float
}

pub struct TransformerGrads {
  d_token_embedding : Tensor
  d_pos_embedding : Tensor
  d_blocks : Array[TransformerBlockGrads]
  d_ln_final_gamma : Tensor
  d_ln_final_beta : Tensor
  d_lm_head : Tensor
}

pub struct TransformerLmCheckpoint {
  params : TransformerParams
  adamw_state : TransformerAdamwState?
  global_step : Int
}

pub struct TransformerLmStepMetric {
  step_ns : UInt64
  loss : Float
  perplexity : Float
}

pub(all) struct TransformerParams {
  token_embedding : Tensor
  pos_embedding : Tensor
  blocks : Array[TransformerBlockParams]
  ln_final_gamma : Tensor
  ln_final_beta : Tensor
  lm_head : Tensor
}

pub struct VitCache {
  patches : Tensor
  patch_embedded : Tensor
  x_with_cls : Tensor
  x_with_pos : Tensor
  block_caches : Array[TransformerBlockCache]
  hidden_before_final_ln : Tensor
  ln_final_mean : FixedArray[Float]
  ln_final_rstd : FixedArray[Float]
  ln_final_out : Tensor
  cls_features : Tensor
  logits : Tensor
}

pub struct VitConfig {
  image_size : Int
  patch_size : Int
  num_patches : Int
  seq_len : Int
  patch_dim : Int
  d_model : Int
  num_heads : Int
  num_layers : Int
  d_ff : Int
  num_classes : Int
  eps : Float
}

pub struct VitGrads {
  d_patch_proj_w : Tensor
  d_patch_proj_b : Tensor
  d_cls_token : Tensor
  d_pos_embedding : Tensor
  d_blocks : Array[TransformerBlockGrads]
  d_ln_final_gamma : Tensor
  d_ln_final_beta : Tensor
  d_cls_head_w : Tensor
  d_cls_head_b : Tensor
}

pub(all) struct VitParams {
  patch_proj_w : Tensor
  patch_proj_b : Tensor
  cls_token : Tensor
  pos_embedding : Tensor
  blocks : Array[TransformerBlockParams]
  ln_final_gamma : Tensor
  ln_final_beta : Tensor
  cls_head_w : Tensor
  cls_head_b : Tensor
}

pub struct VitProfileResult {
  fwd_extract_patches_ns : UInt64
  fwd_patch_linear_ns : UInt64
  fwd_cls_pos_ns : UInt64
  fwd_ln1_ns : UInt64
  fwd_qkv_linear_ns : UInt64
  fwd_reshape_heads_ns : UInt64
  fwd_attn_scores_ns : UInt64
  fwd_softmax_ns : UInt64
  fwd_attn_v_ns : UInt64
  fwd_reshape_back_ns : UInt64
  fwd_wo_linear_ns : UInt64
  fwd_residual1_ns : UInt64
  fwd_ln2_ns : UInt64
  fwd_ff_linear_ns : UInt64
  fwd_gelu_ns : UInt64
  fwd_residual2_ns : UInt64
  fwd_final_ln_cls_ns : UInt64
  bwd_loss_ce_ns : UInt64
  bwd_cls_head_ns : UInt64
  bwd_final_ln_ns : UInt64
  bwd_ffn_ns : UInt64
  bwd_ln2_ns : UInt64
  bwd_mha_ns : UInt64
  bwd_ln1_ns : UInt64
  bwd_residual_ns : UInt64
  bwd_patch_ns : UInt64
  sgd_update_ns : UInt64
  total_ns : UInt64
}

// Type aliases

// Traits

